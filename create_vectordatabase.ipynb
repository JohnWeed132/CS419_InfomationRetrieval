{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from llama_index.core import StorageContext, VectorStoreIndex\n",
    "from llama_index.core.schema import TextNode \n",
    "\n",
    "import qdrant_client\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS IN THE TERMINAL\n",
    "# docker pull qdrant/qdrant\n",
    "# docker run -p 6333:6333 -p 6334:6334 -v \"$(pwd)/vector_database:/qdrant/storage:z\" qdrant/qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv(\"data/corpus.csv\")\n",
    "corpus[\"metadata\"] = corpus.apply(lambda row: {\"document\": row[\"document\"], \"article\": row[\"article\"]}, axis=1)\n",
    "\n",
    "nodes = []\n",
    "for _, row in corpus.iterrows():\n",
    "    node = TextNode(text=row['context'], metadata={\"document\": row['document'], \"article\": row['article']})\n",
    "    nodes.append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = qdrant_client.QdrantClient(\n",
    "    host=\"localhost\",\n",
    "    port=6333\n",
    ")\n",
    "try:\n",
    "    response = client.get_collections()\n",
    "    print(\"Connected to Qdrant successfully!\")\n",
    "    print(\"Available collections:\", response)\n",
    "except Exception as e:\n",
    "    print(\"Failed to connect to Qdrant:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Halong "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_halong = HuggingFaceEmbedding(model_name=\"hiieu/halong_embedding\")\n",
    "vector_halong = QdrantVectorStore(\n",
    "    \"uit_halong\",\n",
    "    client=client,\n",
    "    enable_hybrid=True,\n",
    "    embed_model = model_halong\n",
    ")\n",
    "storage_halong = StorageContext.from_defaults(vector_store=vector_halong)\n",
    "index_halong = VectorStoreIndex(nodes, storage_context=storage_halong,embed_model = model_halong)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve halong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_improve_halong = HuggingFaceEmbedding(model_name=\"johnweak132/improve_halong\")\n",
    "vector_improve_halong = QdrantVectorStore(\n",
    "    \"uit_improve_halong\",\n",
    "    client=client,\n",
    "    enable_hybrid=True,\n",
    "    embed_model = model_improve_halong\n",
    ")\n",
    "storage_improve_halong = StorageContext.from_defaults(vector_store=vector_improve_halong)\n",
    "index_improve_halong = VectorStoreIndex(nodes, storage_context=storage_improve_halong,embed_model = model_improve_halong)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bkai vibi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bkai = HuggingFaceEmbedding(model_name=\"bkai-foundation-models/vietnamese-bi-encoder\")\n",
    "vector_bkai = QdrantVectorStore(\n",
    "    \"uit_bkai\",\n",
    "    client=client,\n",
    "    enable_hybrid=True,\n",
    "    embed_model = model_bkai\n",
    ")\n",
    "storage_bkai = StorageContext.from_defaults(vector_store=vector_bkai)\n",
    "index_bkai = VectorStoreIndex(nodes, storage_context=storage_bkai,embed_model = model_bkai)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve vibi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_improve_bkai = HuggingFaceEmbedding(model_name=\"johnweak132/improve_vibi\")\n",
    "vector_improve_bkai = QdrantVectorStore(\n",
    "    \"uit_improve_bkai\",\n",
    "    client=client,\n",
    "    enable_hybrid=True,\n",
    "    embed_model = model_improve_bkai\n",
    ")\n",
    "storage_improve_bkai = StorageContext.from_defaults(vector_store=vector_improve_bkai)\n",
    "index_improve_bkai = VectorStoreIndex(nodes, storage_context=storage_improve_bkai,embed_model = model_improve_bkai)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
